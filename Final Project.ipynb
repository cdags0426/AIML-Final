{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496dac71",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b331ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import requests\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976f607",
   "metadata": {},
   "source": [
    "Define readability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62d59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in string.punctuation)\n",
    "\n",
    "def syllablesPerWord(word):\n",
    "    vowels = 'aeiouy'\n",
    "    diphthongs = [\"oi\", \"oy\", \"ou\", \"ow\", \"ai\", \"au\", \"ay\", \"aw\", \"oo\", \"ie\", \"ea\", \"ee\"]\n",
    "    word = word.lower()\n",
    "    syllable_count = 0\n",
    "    for i, char in enumerate(word):\n",
    "        if char in vowels:\n",
    "            if i > 0 and word[i-1:i+1] in diphthongs:\n",
    "                continue\n",
    "            syllable_count += 1\n",
    "    if word.endswith('e') and len(word) > 1 and word[-2] not in vowels:\n",
    "        syllable_count -= 1\n",
    "    if len(word) > 2 and word.endswith(\"le\") and word[-3] not in vowels:\n",
    "        syllable_count += 1\n",
    "    return max(1, syllable_count)\n",
    "\n",
    "def getNumberOfTotalWords(s): return len(remove_punctuation(s).split())\n",
    "def getNumberOfTotalSentences(s): return sum(s.count(c) for c in '.!?')\n",
    "def getNumberOfTotalSyllables(s): return sum(syllablesPerWord(w) for w in remove_punctuation(s).split())\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    words = getNumberOfTotalWords(text)\n",
    "    sentences = getNumberOfTotalSentences(text)\n",
    "    syllables = getNumberOfTotalSyllables(text)\n",
    "    return 206.835 - 1.015 * (words / sentences) - 84.6 * (syllables / words)\n",
    "\n",
    "def flesch_kincaid_grade_level(text):\n",
    "    words = getNumberOfTotalWords(text)\n",
    "    sentences = getNumberOfTotalSentences(text)\n",
    "    syllables = getNumberOfTotalSyllables(text)\n",
    "    return 0.39 * (words / sentences) + 11.8 * (syllables / words) - 15.59\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77dda4",
   "metadata": {},
   "source": [
    "Download all of the federalist papers from a outside URL and parse the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9005caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/1404/pg1404.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "\n",
    "\n",
    "chunks = re.split(r\"\\nFEDERALIST No\\. (\\d+)\", text)[1:]\n",
    "essay_data = [(int(chunks[i]), chunks[i+1]) for i in range(0, len(chunks), 2)]\n",
    "essays_df = pd.DataFrame(essay_data, columns=[\"Essay_Number\", \"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f139cfa",
   "metadata": {},
   "source": [
    "Compute the readability metrics and assign authorship labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa2217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_df[\"Flesch_Reading_Ease\"] = essays_df[\"Text\"].apply(flesch_reading_ease)\n",
    "essays_df[\"Flesch_Kincaid_Grade\"] = essays_df[\"Text\"].apply(flesch_kincaid_grade_level)\n",
    "\n",
    "def assign_author(n):\n",
    "    if n in [1,6,7,8,9,11,12,13,15,16,17,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,59,60,61,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85]:\n",
    "        return 'Hamilton'\n",
    "    elif n in [10,14,18,19,20,37,38,39,40,41,42,43,44,45,46,47,48]:\n",
    "        return 'Madison'\n",
    "    elif n in [2,3,4,5]:\n",
    "        return 'Jay'\n",
    "    elif n in range(49,59) or n in [62,63]:\n",
    "        return 'Disputed'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "essays_df[\"Author\"] = essays_df[\"Essay_Number\"].apply(assign_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f245e",
   "metadata": {},
   "source": [
    "Load the 1-gram word frequencies, get the top 20 most frequent words, and merge the text metrics with frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4aec31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onegram_df = pd.read_csv(\"lexos_1gram_inALL_prop.csv\")\n",
    "onegram_df = onegram_df[~onegram_df[\"Unnamed: 0\"].isin([\"Total\", \"Average\"])].copy()\n",
    "onegram_df.rename(columns={\"Unnamed: 0\": \"doc_id\"}, inplace=True)\n",
    "onegram_df[\"Essay_Number\"] = onegram_df[\"doc_id\"].str.extract(r\"FED_(\\d+)_\")[0].astype(int)\n",
    "onegram_df.set_index(\"Essay_Number\", inplace=True)\n",
    "\n",
    "\n",
    "top_20_words = onegram_df.drop(columns=[\"doc_id\"]).mean().sort_values(ascending=False).head(20).index.tolist()\n",
    "freqs_df = onegram_df[top_20_words]\n",
    "\n",
    "\n",
    "full_df = essays_df.merge(freqs_df, left_on=\"Essay_Number\", right_index=True, how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32487b5f",
   "metadata": {},
   "source": [
    "KNN clasifyer of known authors, and predict authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349f5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Flesch_Reading_Ease\", \"Flesch_Kincaid_Grade\"] + top_20_words\n",
    "train = full_df[full_df[\"Author\"].isin([\"Hamilton\", \"Madison\", \"Jay\"])]\n",
    "X_train = train[features]\n",
    "y_train = train[\"Author\"]\n",
    "\n",
    "\n",
    "disputed = full_df[full_df[\"Essay_Number\"].isin([49,50,51,52,53,54,55,56,57,58,62,63])]\n",
    "X_test = disputed[features]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "disputed = disputed.copy()\n",
    "disputed[\"Predicted_Author\"] = knn.predict(X_test)\n",
    "disputed.set_index(\"Essay_Number\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278ce0",
   "metadata": {},
   "source": [
    "Display the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0023f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_Author</th>\n",
       "      <th>Flesch_Reading_Ease</th>\n",
       "      <th>Flesch_Kincaid_Grade</th>\n",
       "      <th>the</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>and</th>\n",
       "      <th>in</th>\n",
       "      <th>a</th>\n",
       "      <th>be</th>\n",
       "      <th>...</th>\n",
       "      <th>which</th>\n",
       "      <th>as</th>\n",
       "      <th>by</th>\n",
       "      <th>this</th>\n",
       "      <th>would</th>\n",
       "      <th>will</th>\n",
       "      <th>or</th>\n",
       "      <th>for</th>\n",
       "      <th>have</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Essay_Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>28.430984</td>\n",
       "      <td>16.147942</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>27.776467</td>\n",
       "      <td>15.748209</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>25.169190</td>\n",
       "      <td>17.294727</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>25.972167</td>\n",
       "      <td>17.232108</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>24.573824</td>\n",
       "      <td>17.480552</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>29.733111</td>\n",
       "      <td>16.602070</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>27.964034</td>\n",
       "      <td>17.489990</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>26.771506</td>\n",
       "      <td>16.955895</td>\n",
       "      <td>0.0871</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>33.681784</td>\n",
       "      <td>15.443767</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Madison</td>\n",
       "      <td>24.075758</td>\n",
       "      <td>17.873519</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>29.556374</td>\n",
       "      <td>16.157086</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>19.347279</td>\n",
       "      <td>19.382849</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Predicted_Author  Flesch_Reading_Ease  Flesch_Kincaid_Grade  \\\n",
       "Essay_Number                                                               \n",
       "49                   Hamilton            28.430984             16.147942   \n",
       "50                   Hamilton            27.776467             15.748209   \n",
       "51                   Hamilton            25.169190             17.294727   \n",
       "52                   Hamilton            25.972167             17.232108   \n",
       "53                   Hamilton            24.573824             17.480552   \n",
       "54                   Hamilton            29.733111             16.602070   \n",
       "55                   Hamilton            27.964034             17.489990   \n",
       "56                   Hamilton            26.771506             16.955895   \n",
       "57                   Hamilton            33.681784             15.443767   \n",
       "58                    Madison            24.075758             17.873519   \n",
       "62                   Hamilton            29.556374             16.157086   \n",
       "63                   Hamilton            19.347279             19.382849   \n",
       "\n",
       "                 the      of      to     and      in       a      be  ...  \\\n",
       "Essay_Number                                                          ...   \n",
       "49            0.1077  0.0611  0.0351  0.0254  0.0206  0.0200  0.0284  ...   \n",
       "50            0.0911  0.0541  0.0252  0.0298  0.0252  0.0126  0.0280  ...   \n",
       "51            0.1052  0.0588  0.0260  0.0208  0.0260  0.0239  0.0328  ...   \n",
       "52            0.1005  0.0546  0.0389  0.0200  0.0178  0.0189  0.0249  ...   \n",
       "53            0.0894  0.0590  0.0336  0.0286  0.0207  0.0221  0.0235  ...   \n",
       "54            0.1017  0.0584  0.0304  0.0190  0.0324  0.0175  0.0200  ...   \n",
       "55            0.0891  0.0597  0.0382  0.0235  0.0147  0.0235  0.0250  ...   \n",
       "56            0.0871  0.0712  0.0248  0.0337  0.0197  0.0305  0.0229  ...   \n",
       "57            0.0976  0.0682  0.0334  0.0244  0.0181  0.0167  0.0194  ...   \n",
       "58            0.1019  0.0560  0.0292  0.0225  0.0278  0.0244  0.0249  ...   \n",
       "62            0.0803  0.0606  0.0335  0.0289  0.0209  0.0314  0.0255  ...   \n",
       "63            0.0953  0.0578  0.0289  0.0223  0.0223  0.0191  0.0197  ...   \n",
       "\n",
       "               which      as      by    this   would    will      or     for  \\\n",
       "Essay_Number                                                                   \n",
       "49            0.0121  0.0091  0.0091  0.0030  0.0133  0.0006  0.0060  0.0067   \n",
       "50            0.0081  0.0099  0.0099  0.0063  0.0099  0.0063  0.0072  0.0054   \n",
       "51            0.0073  0.0146  0.0120  0.0068  0.0047  0.0120  0.0036  0.0068   \n",
       "52            0.0113  0.0086  0.0119  0.0081  0.0043  0.0054  0.0038  0.0070   \n",
       "53            0.0106  0.0101  0.0143  0.0069  0.0028  0.0115  0.0055  0.0097   \n",
       "54            0.0125  0.0180  0.0130  0.0105  0.0030  0.0075  0.0045  0.0050   \n",
       "55            0.0142  0.0059  0.0069  0.0059  0.0049  0.0098  0.0098  0.0083   \n",
       "56            0.0121  0.0064  0.0064  0.0083  0.0025  0.0184  0.0019  0.0045   \n",
       "57            0.0136  0.0108  0.0113  0.0059  0.0027  0.0108  0.0099  0.0086   \n",
       "58            0.0129  0.0100  0.0105  0.0067  0.0057  0.0177  0.0053  0.0062   \n",
       "62            0.0134  0.0067  0.0117  0.0071  0.0021  0.0071  0.0050  0.0067   \n",
       "63            0.0118  0.0099  0.0168  0.0056  0.0036  0.0062  0.0053  0.0085   \n",
       "\n",
       "                have     not  \n",
       "Essay_Number                  \n",
       "49            0.0030  0.0079  \n",
       "50            0.0108  0.0090  \n",
       "51            0.0016  0.0047  \n",
       "52            0.0103  0.0059  \n",
       "53            0.0032  0.0088  \n",
       "54            0.0080  0.0090  \n",
       "55            0.0029  0.0069  \n",
       "56            0.0051  0.0070  \n",
       "57            0.0050  0.0068  \n",
       "58            0.0086  0.0067  \n",
       "62            0.0029  0.0071  \n",
       "63            0.0069  0.0085  \n",
       "\n",
       "[12 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"Predicted_Author\", \"Flesch_Reading_Ease\", \"Flesch_Kincaid_Grade\"] + top_20_words\n",
    "display(disputed[cols].sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d90a4",
   "metadata": {},
   "source": [
    "Looks like most of the disputed papers lean toward Hamilton based on this method, with only Paper 58 siding with Madison. That said, this analysis only used readability scores and the top 20 most frequent words — nothing super complex. Still, it’s interesting how even these simple features hint at authorship patterns. It’s definitely not the full story, nor does it agree too strongly with the more widely accepted authorship predictions, but it shows that writing style can be pretty telling, even with just surface-level stats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
